from __future__ import division
import pandas as pd
import argparse
import warnings
from tqdm import tnrange
import torch
from torch import nn
import pandas as pd
import numpy as np
from sklearn import preprocessing
import torch
from torch.autograd import Variable
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from torch.nn import init
from torch.utils.data import TensorDataset, DataLoader
from torchvision.transforms import ToTensor
from torch.utils.data import TensorDataset, DataLoader
from torch.autograd import Variable
import math
from torch import optim
from torch.nn import init
import torch.optim as optim
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
import math
import torch.nn.functional as F
import pdb

####Define loss
def Entropy(input_):
    bs = input_.size(0)
    epsilon = 1e-5
    entropy = -input_ * torch.log(input_ + epsilon)
    entropy = torch.sum(entropy, dim=1)
    return entropy

def grl_hook(coeff):
    def fun1(grad):
        return -coeff*grad.clone()
    return fun1

def CDAN(input_list, ad_net, entropy=None, coeff=None, random_layer=None):
    softmax_output = input_list[1].detach()
    feature = input_list[0]
    op_out = torch.bmm(softmax_output.unsqueeze(2), feature.unsqueeze(1))
    ad_out = ad_net(op_out.view(-1, softmax_output.size(1) * feature.size(1)))   
    batch_size = softmax_output.size(0) // 2
    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).to(torch.float32)
    if entropy is not None:
        entropy.register_hook(grl_hook(coeff))
        entropy = 1.0+torch.exp(-entropy)
        source_mask = torch.ones_like(entropy)
        source_mask[feature.size(0)//2:] = 0
        source_weight = entropy*source_mask
        target_mask = torch.ones_like(entropy)
        target_mask[0:feature.size(0)//2] = 0
        target_weight = entropy*target_mask
        weight = source_weight / torch.sum(source_weight).detach().item() + \
                 target_weight / torch.sum(target_weight).detach().item()
        return torch.sum(weight.view(-1, 1) * nn.BCELoss(reduction='none')(ad_out, dc_target)) / torch.sum(weight).detach().item()
    else:
        return nn.BCELoss()(ad_out, dc_target) 


def DANN(features, ad_net):
    ad_out = ad_net(features)
    batch_size = ad_out.size(0) // 2
    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).to(torch.float32)
    return nn.BCELoss()(ad_out, dc_target)


###Saving log and load model
from skimage import io
import pickle
import numpy as np
import matplotlib.pyplot as plt
import os
import torch
from torchvision import transforms, datasets
try:
	from torch.hub import load_state_dict_from_url
except ImportError:
	from torch.utils.model_zoo import load_url as load_state_dict_from_url
    
def save_log(obj, path):
	with open(path, 'wb') as f:
		pickle.dump(obj, f)
		print('[INFO] Object saved to {}'.format(path))

def save_model(model, path):
	torch.save(model.state_dict(), path)
	print("checkpoint saved in {}".format(path))

def load_model(model, path):
	"""
	Loads trained network params in case AlexNet params are not loaded.
	"""
	model.load_state_dict(torch.load(path))
	print("pre-trained model loaded from {}".format(path))

def get_mean_std_dataset(root_dir):
	"""
	Function to compute mean and std of image dataset.
	Move batch_size param according to memory resources.
	retrieved from: https://forums.fast.ai/t/image-normalization-in-pytorch/7534/7
	"""

	# data_domain = "amazon"
	# path_dataset = "datasets/office/%s/images" % data_domain

	transform = transforms.Compose([
	transforms.Resize((224, 224)), # original image size 300x300 pixels
	transforms.ToTensor()])

	dataset = datasets.ImageFolder(root=root_dir,
	                               transform=transform)

	# set large batch size to get good approximate of mean, std of full dataset
	# batch_size: 4096, 2048
	data_loader = DataLoader(dataset, batch_size=2048,
	                        shuffle=False, num_workers=0)

	mean = []
	std = []

	for i, data in enumerate(data_loader, 0):
	    # shape is (batch_size, channels, height, width)
	    npy_image = data[0].numpy()

	    # compute mean, std per batch shape (3,) three channels
	    batch_mean = np.mean(npy_image, axis=(0,2,3))
	    batch_std = np.std(npy_image, axis=(0,2,3))

	    mean.append(batch_mean)
	    std.append(batch_std)

	# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)
	mean = np.array(mean).mean(axis=0) # average over batch averages
	std = np.arry(std).mean(axis=0) # average over batch stds

	values = {
	    "mean": mean,
	    "std": std
	}

	return values
  
  
###Define Model

def init_weights(m):
	classname = m.__class__.__name__
	if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:
		nn.init.kaiming_uniform_(m.weight)
		nn.init.zeros_(m.bias)
	elif classname.find('BatchNorm') != -1:
		nn.init.normal_(m.weight, 1.0, 0.02)
		nn.init.zeros_(m.bias)
	elif classname.find('Linear') != -1:
		nn.init.xavier_normal_(m.weight)
		nn.init.zeros_(m.bias)


def calc_coeff(iter_num, high=1.0, low=0.0, alpha=10.0, max_iter=10000.0):
	return np.float(2.0 * (high - low) / (1.0 + np.exp(-alpha * iter_num / max_iter)) - (high - low) + low)


def grl_hook(coeff):
	def fun1(grad):
		return -coeff * grad.clone()

	return fun1

class RandomLayer(nn.Module):
    def __init__(self, input_dim_list=[], output_dim=1024):
        super(RandomLayer, self).__init__()
        self.input_num = len(input_dim_list)
        self.output_dim = output_dim
        self.random_matrix = [torch.randn(input_dim_list[i], output_dim) for i in range(self.input_num)]

    def forward(self, input_list):
        return_list = [torch.mm(input_list[i], self.random_matrix[i]) for i in range(self.input_num)]
        return_tensor = return_list[0] / math.pow(float(self.output_dim), 1.0/len(return_list))
        for single in return_list[1:]:
            return_tensor = torch.mul(return_tensor, single)
        return return_tensor

    def cuda(self):
        super(RandomLayer, self).cuda()
        self.random_matrix = [val.cuda() for val in self.random_matrix]

class baseNetwork(nn.Module):
    def __init__(self, num_classes=2):
        super(baseNetwork, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(21,128),
            nn.ReLU(inplace=True),
            nn.Linear(128,128),
            nn.ReLU(inplace=True),
            nn.Linear(128,128),
						nn.ReLU(inplace=True),
            nn.Linear(128,128),
            nn.ReLU(inplace=True),
						#nn.Linear(128,128),
            #nn.ReLU(inplace=True),
            )
        
        self.bottleneck = nn.Sequential(
            nn.Linear(128,64),
            nn.ReLU(inplace=True),
            )

        self.fc8 = nn.Sequential(
            nn.Linear(64, 2)
            )

    def forward(self, source): # computes activations for BOTH domains
        features = self.classifier(source)
        features = self.bottleneck(features)
        outputs = self.fc8(features)
        
        return features, outputs




class AdversarialNetwork(nn.Module):
	"""
    AdversarialNetwork obtained from official CDAN repository:
    https://github.com/thuml/CDAN/blob/master/pytorch/network.py
    """
	def __init__(self, in_feature, hidden_size):
		super(AdversarialNetwork, self).__init__()

		self.ad_layer1 = nn.Linear(in_feature, hidden_size)
		self.ad_layer2 = nn.Linear(hidden_size, hidden_size)
		self.ad_layer3 = nn.Linear(hidden_size, 1)
		self.relu1 = nn.ReLU()
		self.relu2 = nn.ReLU()
		self.dropout1 = nn.Dropout(0.5)
		self.dropout2 = nn.Dropout(0.5)
		self.sigmoid = nn.Sigmoid()
		self.apply(init_weights)
		self.iter_num = 0
		self.alpha = 10
		self.low = 0.0
		self.high = 1.0
		self.max_iter = 10000.0

	def forward(self, x):
		#print("inside ad net forward",self.training)
		if self.training:
			self.iter_num += 1
		coeff = calc_coeff(self.iter_num, self.high, self.low, self.alpha, self.max_iter)
		x = x * 1.0
		x.register_hook(grl_hook(coeff))
		x = self.ad_layer1(x)
		x = self.relu1(x)
		x = self.dropout1(x)
		x = self.ad_layer2(x)
		x = self.relu2(x)
		x = self.dropout2(x)
		y = self.ad_layer3(x)
		y = self.sigmoid(y)
		return y


	def output_num(self):
		return 1

	def get_parameters(self):
		return [{"params": self.parameters(), "lr_mult": 10, 'decay_mult': 2}]
    
    
    
  ##Upload data
  ###dataloader
####Load data to tensor
special_80=pd.read_csv('special_80.csv')
special_80=list(special_80['行业80'])
data_test=pd.read_csv('data_newuser_80_pre.csv')
data_train=pd.read_csv('data_train_pre.csv')
fake_data=pd.read_csv('fake_data.csv')

data_test=data_test.iloc[:,1:]
data_train=data_train.iloc[:,1:]
fake_data=fake_data.iloc[:,1:]

x = data_train.iloc[0:82176,1:-1]
y = data_train.iloc[0:82176, -1]
source_d, validation_d, source_l, validation_l = train_test_split(x, y, test_size=0.2,random_state=42)
target_test_d=np.array(data_test.iloc[0:5632,1:-1])
target_test_l=np.array(data_test.iloc[0:5632,-1])
target_train_d=np.array(fake_data.iloc[0:65792,1:-1])
target_train_l=np.array(fake_data.iloc[0:65792,-1])


source_d=torch.tensor(np.array(source_d)).to(dtype=torch.float32)
source_l=torch.tensor(np.array(source_l)).to(dtype=torch.float32)
target_train_d=torch.tensor(target_train_d).to(dtype=torch.float32)
target_train_l=torch.tensor(target_train_l).to(dtype=torch.float32)
target_test_d=torch.tensor(target_test_d).to(dtype=torch.float32)
target_test_l=torch.tensor(target_test_l).to(dtype=torch.float32)
source_l.size()

BATCH_SIZE=256
dataset=TensorDataset(source_d,source_l)
source_loader=DataLoader(dataset=dataset,batch_size=BATCH_SIZE,shuffle=True)

for i, dataset in enumerate(source_loader):
    data,label=dataset


dataset=TensorDataset(target_train_d,target_train_l)
target_train_loader=DataLoader(dataset=dataset,batch_size=BATCH_SIZE,shuffle=True)
for i, dataset in enumerate(target_train_loader):
    data,label=dataset

dataset=TensorDataset(target_test_d,target_test_l)
target_test_loader=DataLoader(dataset=dataset,batch_size=BATCH_SIZE,shuffle=True)
for i, dataset in enumerate(target_test_loader):
    data,label=dataset


####Train model
# set model hyperparameters (paper page 5)
learning_rate = 0.01
L2_DECAY = 5e-4
MOMENTUM = 0.9
EPOCHS=150
log_interval=500
training_s_statistic =[]
testing_s_statistic = []
testing_t_statistic = []
results_test = []
results_train = []
class_weights=torch.tensor([0.25,0.75])
clf_criterion=nn.CrossEntropyLoss(weight=class_weights)


#实例化
model = baseNetwork()
ad_net = AdversarialNetwork(64,128)

def initialize(self):
    for m in self.modules():
        if isinstance(m, nn.Linear):
            init.xavier_uniform_(m.weight.data) 
model.apply(initialize)
ad_net.apply(initialize)


####Train
def train(model, ad_net, train_loader, target_train_loader, epoch, start_epoch):
    """
    This method fits the network params one epoch at a time.
    Implementation based on:
    https://github.com/SSARCandy/DeepCORAL/blob/master/main.py
    """
    model.train()
    correct_source = 0
    FN=0
    TN=0
    FP=0
    TP=0
    # LEARNING_RATE = step_decay(epoch, learning_rate)
    # optimizer = torch.optim.SGD([
    #     {"params": model.classifier.parameters(),"lr":LEARNING_RATE},
    #     {"params": model.bottleneck.parameters(),"lr":LEARNING_RATE},
    #     {"params": model.fc8.parameters(), "lr":LEARNING_RATE},
    #     {"params":ad_net.parameters(), "lr_mult": 10, 'decay_mult': 2}
    # ], lr=LEARNING_RATE,momentum=MOMENTUM)#momentum=MOMENTUM,
    # LEARNING_RATE = step_decay(epoch, learning_rate)
    LEARNING_RATE=learning_rate
    
    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0005, momentum=0.9)
    optimizer_ad = optim.SGD(ad_net.parameters(), lr=LEARNING_RATE, weight_decay=0.0005, momentum=0.9)
    # optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_DECAY,betas=(0.9, 0.999))
    # optimizer_ad = optim.Adam(ad_net.parameters(), lr=LEARNING_RATE, weight_decay=L2_DECAY,betas=(0.9, 0.999))

    iter_source = iter(source_loader)
    iter_target_train = iter(target_train_loader)
    train_steps =  min(len(source_loader),len(target_train_loader))

    # start batch training
    for i in range(0,train_steps):
        # fetch data in batches
        source_data,source_label = next(iter_source)
        target_data, target_label = next(iter_target_train)
        # ##保证数据能够整除Batch
        if i % len(target_train_loader) == 0:
            iter_target_train = iter(target_train_loader)
        if i % len(source_loader)==0:
            iter_source=iter(source_loader)

        # create pytorch variables, the variables and functions build a dynamic graph of computation
        source_data, source_label = Variable(source_data), Variable(source_label)
        target_data = Variable(target_data)

        # reset to zero optimizer gradients
        optimizer.zero_grad()
        optimizer_ad.zero_grad()
        
        lambda_factor=(epoch+1)/EPOCHS

        # do a forward pass through network (recall DeepCORAL outputs source, target activation maps)
        src_features, src_ouputs = model(source_data)
        tgt_features, tgt_ouputs = model(target_data)
        feature = torch.cat((src_features, tgt_features), dim=0)
        output = torch.cat((src_ouputs, tgt_ouputs), dim=0)
        softmax_output = nn.Softmax(dim=1)(output)
        # entropy = Entropy(softmax_output)
        pred = src_ouputs.data.max(1, keepdim=True)[1]

        classification_loss = clf_criterion(output.narrow(0, 0, source_data.size(0)), source_label.to(dtype=torch.long))
        
        
        zes=Variable(torch.zeros(BATCH_SIZE).type(torch.LongTensor))
        ons=Variable(torch.ones(BATCH_SIZE).type(torch.LongTensor))
        train_correct01 = ((pred.squeeze(1)==zes)&(source_label==ons)).sum() 
        train_correct10 = ((pred.squeeze(1)==ons)&(source_label==zes)).sum() 
        train_correct11 = ((pred.squeeze(1)==ons)&(source_label==ons)).sum() 
        train_correct00 = ((pred.squeeze(1)==zes)&(source_label==zes)).sum() 
        FN += train_correct01
        FP += train_correct10
        TP += train_correct11
        TN += train_correct00
        correct_source += pred.eq(source_label.data.view_as(pred)).cpu().sum()

        #transfer_loss = CDAN([feature, softmax_output], ad_net,None,None,None)
        transfer_loss = DANN(feature, ad_net)


        # compute total loss
        total_loss = classification_loss + lambda_factor*transfer_loss
        
        # compute gradients of network
        total_loss.backward()

        # update weights of network
        optimizer.step()
        
        if epoch > 1:
            optimizer_ad.step()
            
        # print training info
        if i % log_interval == 0:
            print('Train Epoch: {:2d} [{:2d}/{:2d}]\t'
                  'Classification loss: {:.6f}, transfer loss: {:.6f}, Total_Loss: {:.6f}'.format(
                      epoch,
                      i + 1,
                      train_steps,
                      # lambda_factor,
                      classification_loss.item(), # classification_loss.data[0],
                      transfer_loss.item(),
                      total_loss.item() # total_loss.data[0]
                  ))

        # append results for each batch iteration as dictionaries
        results_train.append({
            'epoch': epoch,
            'step': i+ 1,
            'total_steps': train_steps,
            'transfer_loss': transfer_loss.item(), 
            'classification_loss': classification_loss.item(),  # classification_loss.data[0],
            'total_loss': total_loss.item() # total_loss.data[0]
        })


    total_loss /= len(source_loader)
    acc_source = float(correct_source) * 100. / (len(source_loader) * BATCH_SIZE)
    recall=TP/(TP+FN)
    precision=TP/(TP+FP)
    F1=2*(recall*precision)/(recall+precision)
    

    print('Source_set: Average classification loss: {:.4f}, Accuracy source:{:.2f}%,Recall:{}, F1:{},Precision:{}'.format(
       total_loss, acc_source,recall,F1,precision))
    
    return results_train

def test(model, target_test_loader, epoch):
    """
    Computes classification accuracy of (labeled) data using cross-entropy.
    Retreived from: https://github.com/SSARCandy/DeepCORAL/blob/master/main.py
    """
    # eval() it indicates the model that nothing new is
    # to be learnt and the model is used for testing
    FN_test= 0
    TN_test= 0
    FP_test= 0
    TP_test= 0
    test_loss = 0
    correct_class=0
    model.eval()

    test_loss = 0
    correct_class = 0
    
    with torch.no_grad():

    # go over dataloader batches, labels
        for data, label in target_test_loader:

        # note on volatile: https://stackoverflow.com/questions/49837638/what-is-volatile-variable-in-pytorch
            data, label = Variable(data, volatile=True), Variable(label)
            feature, output = model(data) # just use one ouput of DeepCORAL

        # sum batch loss when computing classification
            test_loss += clf_criterion(output, label.to(dtype=torch.long)).item()
            # test_loss += nn.CrossEntropyLoss(weight=class_weights)(output, label.to(dtype=torch.long))

        # get the index of the max log-probability
            pred = output.data.max(1, keepdim=True)[1]
            correct_class += pred.eq(label.data.view_as(pred)).sum().item()

    # compute test loss as correclty classified labels divided by total data size
        test_loss /= len(target_test_loader.dataset)
        acc_test=100.*correct_class/len(target_test_loader.dataset)
        
        # return dictionary containing info of each epoch
        results_test.append({
            "epoch": epoch,
            "average_loss": test_loss,
            "correct_class": correct_class,
            "total_elems": len(target_test_loader.dataset),
            "accuracy %": acc_test
        })
        print("Test_Accuracy:{}".format(acc_test))
    
        # print("Test_Accuracy:{}/tRecall:{}/tPrecision:{}/tF1:{}/tloss{}".format(acc_test,recall_test,percision_test,F1_test,test_loss))
        
    return results_test


if __name__ == '__main__':

    source_loader=source_loader
    target_train_loader = target_train_loader
    target_test_loader = target_test_loader
    
    # optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_DECAY, momentum=MOMENTUM)
    # optimizer_ad = optim.SGD(ad_net.parameters(), lr=LEARNING_RATE, weight_decay=L2_DECAY, momentum=MOMENTUM)


    # # load pretrained alexnet model
    # ddcnet = model.load_pretrained_alexnet(ddcnet)
    # print('Load pretrained alexnet parameters complete\n')

    for epoch in range(1, EPOCHS+1):

        # run batch trainig at each epoch (returns dictionary with epoch result)
        results_train = train(model, ad_net, source_loader, target_train_loader, epoch, 0)
        training_s_statistic.append(results_train)
        
        results_test = test(model, target_test_loader, epoch)
        testing_t_statistic.append(results_test)
    
    save_log(training_s_statistic, 'adaptation_training_statistic.pkl')
    save_log(testing_t_statistic, 'adaptation_testing_t_statistic.pkl')
    for param_tensor in model.state_dict():
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())
    model_save_path = os.path.join('model.pt')
    torch.save(model.state_dict(), model_save_path)
    
    
 
 ###Prediction and evaluation


def prediction(target_test_loader):
    model.eval()
    with torch.no_grad():
        test_loss = 0
        results_test=[]
        correct= 0
        FN_test= 0
        TN_test= 0
        FP_test= 0
        TP_test= 0
        for test_data, test_target in target_test_loader:
            test_data, test_target = Variable(test_data), Variable(test_target)
            feature,target_test_preds = model(test_data)
            test_pred = target_test_preds.data.max(1, keepdim=True)[1]
            correct += test_pred.eq(test_target.data.view_as(test_pred)).sum().item()
            test_loss += clf_criterion(target_test_preds, test_target.to(dtype=torch.long)).item()
            zes=Variable(torch.zeros(BATCH_SIZE).type(torch.LongTensor))
            ons=Variable(torch.ones(BATCH_SIZE).type(torch.LongTensor))
            test_correct01 = ((test_pred==zes)&(test_target==ons)).sum()
            test_correct10 = ((test_pred==ons)&(test_target==zes)).sum()
            test_correct11 = ((test_pred==ons)&(test_target==ons)).sum()
            test_correct00 = ((test_pred==zes)&(test_target==zes)).sum()
            FN_test += test_correct01.item()
            FP_test += test_correct10.item()
            TP_test += test_correct11.item()
            TN_test += test_correct00.item()

    acc_test=100.*correct/len(target_test_loader.dataset)
    recall_test=TP_test/(TP_test+FN_test+0.1)
    percision_test=TP_test/(TP_test+FP_test+0.1)
    F1_test=2*(recall_test*percision_test)/(recall_test+percision_test+0.1)
    test_loss =test_loss/len(target_test_loader)
    return acc_test,recall_test,percision_test,F1_test,test_loss
    print("Accuracy:{}/tRecall:{}/tPrecision:{}/tF1:{}/tloss{}".format(acc_test,recall_test,percision_test,F1_test,test_loss))
    
####Different testing dataset
####Different testing dataset
industry_cut=(80,60,40,30,20,10)


##Validation
validation_d=torch.tensor(np.array(validation_d)).to(dtype=torch.float32)
validation_l=torch.tensor(np.array(validation_l)).to(dtype=torch.float32)
dataset=TensorDataset(validation_d,validation_l)
validation_loader=DataLoader(dataset=dataset,batch_size=1)
for i, dataset in enumerate(validation_loader):
    data,label=dataset
prediction_val=prediction(validation_loader)
print(prediction_val)

###80
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[0]])]
target_test_d_80=np.array(data_test_i.iloc[:,1:-1])
target_test_l_80=np.array(data_test_i.iloc[:,-1])
target_test_d_80=torch.tensor(target_test_d_80).to(dtype=torch.float32)
target_test_l_80=torch.tensor(target_test_l_80).to(dtype=torch.float32)
dataset_80=TensorDataset(target_test_d_80,target_test_l_80)
target_test_loader_80=DataLoader(dataset=dataset_80,batch_size=1)
for i, dataset in enumerate(target_test_loader_80):
    data,label=dataset
prediction_80=prediction(target_test_loader_80)
prediction(target_test_loader_80)

###60
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[1]])]
target_test_d_60=np.array(data_test_i.iloc[:,1:-1])
target_test_l_60=np.array(data_test_i.iloc[:,-1])
target_test_d_60=torch.tensor(target_test_d_60).to(dtype=torch.float32)
target_test_l_60=torch.tensor(target_test_l_60).to(dtype=torch.float32)
dataset_60=TensorDataset(target_test_d_60,target_test_l_60)
target_test_loader_60=DataLoader(dataset=dataset_60,batch_size=1)
for i, dataset in enumerate(target_test_loader_60):
    data,label=dataset
prediction_60=prediction(target_test_loader_60)
prediction(target_test_loader_60)

###40
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[2]])]
target_test_d_40=np.array(data_test_i.iloc[:,1:-1])
target_test_l_40=np.array(data_test_i.iloc[:,-1])
target_test_d_40=torch.tensor(target_test_d_40).to(dtype=torch.float32)
target_test_l_40=torch.tensor(target_test_l_40).to(dtype=torch.float32)
dataset_40=TensorDataset(target_test_d_40,target_test_l_40)
target_test_loader_40=DataLoader(dataset=dataset_40,batch_size=1)
for i, dataset in enumerate(target_test_loader_40):
    data,label=dataset
prediction_40=prediction(target_test_loader_40)
prediction(target_test_loader_40)

###30
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[3]])]
target_test_d_30=np.array(data_test_i.iloc[:,1:-1])
target_test_l_30=np.array(data_test_i.iloc[:,-1])
target_test_d_30=torch.tensor(target_test_d_30).to(dtype=torch.float32)
target_test_l_30=torch.tensor(target_test_l_30).to(dtype=torch.float32)
dataset_30=TensorDataset(target_test_d_30,target_test_l_30)
target_test_loader_30=DataLoader(dataset=dataset_30,batch_size=1)
for i, dataset in enumerate(target_test_loader_30):
    data,label=dataset
prediction_30=prediction(target_test_loader_30)
prediction(target_test_loader_30)
    
###25
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[4]])]
target_test_d_20=np.array(data_test_i.iloc[:,1:-1])
target_test_l_20=np.array(data_test_i.iloc[:,-1])
target_test_d_20=torch.tensor(target_test_d_20).to(dtype=torch.float32)
target_test_l_20=torch.tensor(target_test_l_20).to(dtype=torch.float32)
dataset_20=TensorDataset(target_test_d_20,target_test_l_20)
target_test_loader_20=DataLoader(dataset=dataset_20,batch_size=1)
for i, dataset in enumerate(target_test_loader_20):
    data,label=dataset
prediction_20=prediction(target_test_loader_20)
prediction(target_test_loader_20)

        
###10
data_test_i=data_test[data_test['segement_industry'].isin(special_80[0:industry_cut[5]])]
target_test_d_10=np.array(data_test_i.iloc[:,1:-1])
target_test_l_10=np.array(data_test_i.iloc[:,-1])
target_test_d_10=torch.tensor(target_test_d_10).to(dtype=torch.float32)
target_test_l_10=torch.tensor(target_test_l_10).to(dtype=torch.float32)
dataset_10=TensorDataset(target_test_d_10,target_test_l_10)
target_test_loader_10=DataLoader(dataset=dataset_10,batch_size=1)
for i, dataset in enumerate(target_test_loader_10):
    data,label=dataset
prediction_10=prediction(target_test_loader_10)
prediction(target_test_loader_10)


loss_lambda=[prediction_val[4],prediction_80[4],prediction_60[4],prediction_40[4],prediction_30[4],prediction_20[4],
      prediction_10[4]]
print(loss_lambda)

recall_f1=[prediction_val[1],prediction_val[3]
           ,prediction_80[1],prediction_80[3]
           ,prediction_60[1],prediction_60[3]
           ,prediction_40[1],prediction_40[3]
           ,prediction_30[1],prediction_30[3]
           ,prediction_20[1],prediction_20[3]
           ,prediction_10[1],prediction_10[3]]
print(recall_f1)
